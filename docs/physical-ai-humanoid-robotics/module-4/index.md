# Module 4: Vision-Language-Action (VLA)

Focus: The convergence of LLMs and Robotics.

In this final module, we will explore the exciting field of Vision-Language-Action (VLA) models and their application to robotics. We will learn how to build a humanoid robot that can understand natural language commands, perceive its environment, and take action to complete complex tasks. We will cover:
*   Voice-to-Action: Using OpenAI Whisper for voice commands.
*   Cognitive Planning: Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions.
*   Capstone Project: The Autonomous Humanoid.
